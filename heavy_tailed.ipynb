{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddingsHT(BertEmbeddings):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        # Apply non-linear transformation (e.g., sigmoid, tanh) to embeddings\n",
    "        # to make them fat-tailed\n",
    "        embeddings = torch.sigmoid(words_embeddings) + torch.tanh(position_embeddings) + torch.relu(token_type_embeddings)\n",
    "\n",
    "        # embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelHT(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = BertEmbeddingsHT(config)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
    "        return super().forward(input_ids, token_type_ids, attention_mask, output_all_encoded_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662804195/662804195 [01:41<00:00, 6535101.58B/s]\n"
     ]
    }
   ],
   "source": [
    "model = BertModelHT.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 995526/995526 [00:00<00:00, 2252454.15B/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary-multilingual)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'am', 'so', 'happy', ',', 'today', 'is', 'my', 'birthday', '!', 'i', 'can', \"'\", 't', 'wait', 'to', 'dance', 'with', 'my', 'friends', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am so happy, today is my birthday! I can't wait to dance with my friends!\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModelHT(\n",
       "  (embeddings): BertEmbeddingsHT(\n",
       "    (word_embeddings): Embedding(119547, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 12, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `encoded_layers` has shape [12 x 1 x 23 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [23 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 23 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.5379e-01, -5.2726e-01,  3.9038e-01,  4.6449e-02,  1.0117e-01,\n",
       "        -2.8333e-01, -4.6904e-01,  1.0536e-01,  4.0712e-01,  4.4933e-01,\n",
       "        -3.7807e-01,  2.4165e-01,  5.9514e-01,  3.8656e-01,  3.4816e-01,\n",
       "        -5.9424e-01,  5.4353e-01, -1.6189e-01,  2.0134e-01,  4.1504e-01,\n",
       "        -9.2185e-02, -3.7987e-02,  7.6645e-01,  6.4855e-01, -2.4880e-01,\n",
       "         4.3472e-01, -4.5661e-01, -9.2187e-03,  1.9247e-01, -4.4045e-01,\n",
       "        -2.9806e-01,  1.5573e-01, -3.5994e-02,  6.9275e-01, -1.5687e-01,\n",
       "         1.4037e-01, -4.2200e-02,  4.2213e-01, -2.5957e-01, -5.3310e-01,\n",
       "         1.6371e-01,  1.5584e-01,  3.9058e-01, -5.5510e-02, -1.5361e-01,\n",
       "        -5.2311e-01,  6.8016e-01,  5.9693e-01, -7.5328e-03,  1.8516e-01,\n",
       "         2.1970e-01, -1.6238e-01,  5.2092e-01,  2.4030e-01,  2.3908e-01,\n",
       "         6.0401e-02,  2.5389e-01, -3.3801e-01,  1.2806e-01,  1.1195e-01,\n",
       "         1.5407e-02,  3.8178e-01,  3.6687e-01, -2.1961e-01, -2.0979e-01,\n",
       "         1.7412e-01,  5.7783e-02,  2.2307e-02,  2.0237e-01, -8.3755e-02,\n",
       "         2.9550e-01,  3.1995e-01,  7.0577e-04,  2.9172e-02,  2.3406e-01,\n",
       "         6.7554e-01,  2.9236e-01, -2.7148e-01,  2.7193e-01, -3.5967e-01,\n",
       "         2.9802e-01, -1.9736e-01,  2.7251e-01, -1.4344e-02,  2.5592e-01,\n",
       "        -2.2801e-01,  1.2360e-01,  3.2415e-01,  4.6017e-01,  1.4859e-01,\n",
       "         4.8514e-02,  3.9437e-01,  5.7236e-01,  2.4923e-01, -4.0840e-01,\n",
       "         4.6164e-01, -1.6627e-01, -1.0610e-01,  1.4476e-01,  1.0444e-01,\n",
       "         3.0557e-01, -6.3532e-02,  6.1545e-01,  8.7227e-02,  3.3950e-01,\n",
       "         1.5650e-01, -1.2316e-01,  8.2420e-02,  6.2727e-01,  1.6848e-01,\n",
       "         4.2952e-01, -9.1040e-02, -2.1920e-01, -3.2666e-01,  1.6114e-01,\n",
       "         2.6135e-01, -3.5007e-01, -5.7727e-01,  4.6002e-01, -2.4410e-01,\n",
       "         3.7602e-01,  3.4362e-01,  1.1811e-01,  9.5386e-02, -6.4071e-01,\n",
       "         3.9555e-01, -2.5773e-01, -5.9224e-02,  1.3229e-01,  3.5210e-01,\n",
       "        -4.9608e-01,  5.2598e-02,  1.1553e-01,  2.5182e-01,  1.6070e-01,\n",
       "         9.7530e-02, -1.0781e-02,  4.6576e-01, -8.3413e-02,  3.6738e-01,\n",
       "        -2.3882e-01,  1.3648e-02,  3.8320e-02, -2.9746e-01, -1.8121e-01,\n",
       "         2.3503e-01, -3.4851e-01, -1.3516e-01,  1.9808e-01, -4.0464e-02,\n",
       "        -1.7272e-01, -6.7870e-02,  3.4762e-01, -3.6029e-01, -8.4095e-01,\n",
       "         1.9397e-01,  5.3998e-01,  1.4602e-01, -2.4396e-01,  4.8912e-01,\n",
       "         5.0740e-02,  5.0678e-02, -6.1208e-01,  2.9165e-01, -2.3359e-01,\n",
       "        -4.1503e-01,  6.1949e-01, -6.5744e-01, -1.9019e-01,  1.4291e-01,\n",
       "         1.3662e-01,  3.0143e-01, -4.1034e-01, -1.6722e-01,  5.3999e-01,\n",
       "         2.5482e-01,  3.1126e-02, -2.8646e-01, -5.0501e-01, -2.6100e-01,\n",
       "         1.5669e-01,  5.2824e-02,  2.9540e-01, -3.1357e-01,  3.0092e-01,\n",
       "         2.7254e-01,  1.2275e-01, -5.6522e-02, -4.2714e-01, -9.8720e-02,\n",
       "        -3.3766e-01,  2.3704e-02, -2.9173e-01,  7.7147e-02, -4.3342e-01,\n",
       "        -3.9620e-02, -1.8892e-01, -4.5424e-02,  2.7449e-01, -4.8953e-02,\n",
       "        -2.3353e-01, -3.4530e-01,  8.6151e-02,  1.2269e-01,  4.9158e-01,\n",
       "         2.7213e-01,  2.9661e-02, -3.1603e-02, -1.3770e-01,  3.2292e-01,\n",
       "         7.8610e-03, -1.6120e-01,  4.2693e-01, -2.2509e-01,  4.3956e-01,\n",
       "        -4.1075e-01, -4.4443e-01, -1.9587e-01,  2.7668e-01, -8.8723e-02,\n",
       "         2.5968e-01, -4.7980e-02, -3.8148e-01, -4.3099e-01, -8.6497e-02,\n",
       "        -1.7427e-01,  5.8836e-02,  2.2731e-01, -2.7879e-01, -2.8598e-01,\n",
       "        -1.9259e-01, -5.0703e-01, -5.8235e-01,  3.9060e-01, -1.3449e-01,\n",
       "         2.0166e-01, -4.1016e-02, -1.0796e-01,  8.1609e-02, -6.7071e-02,\n",
       "         1.6436e-01, -2.0461e-02, -4.2855e-01, -2.0127e-01,  3.6959e-01,\n",
       "        -2.8688e-02, -2.7146e-01, -5.1053e-01, -1.6404e-01, -2.2943e-01,\n",
       "         4.3144e-01, -1.7786e-01,  6.5611e-01, -2.7866e-01, -1.3130e-01,\n",
       "         5.9134e-01, -7.7617e-02,  1.0472e-01, -7.8081e-02, -4.5226e-01,\n",
       "         1.6057e-01, -7.1807e-01, -3.2848e-02,  8.0696e-02,  2.0226e-02,\n",
       "         3.0839e-01,  2.1705e-01,  4.1337e-02,  3.9418e-02,  2.2813e-01,\n",
       "        -2.9363e-02,  2.9415e-01,  7.9013e-02,  3.0644e-01, -1.5295e-01,\n",
       "        -1.7401e-02,  9.5502e-02, -2.1076e-01, -4.0918e-01, -3.0284e-01,\n",
       "        -4.1200e-01, -3.5617e-01,  4.7875e-02, -2.5118e-01,  2.4549e-01,\n",
       "        -7.5223e-01, -5.0368e-02, -3.6678e-01,  1.6430e-01, -2.0245e-01,\n",
       "         2.5231e-01, -1.6721e-01,  6.9162e-02,  1.3654e-01,  6.5263e-03,\n",
       "        -6.1576e-02, -9.0900e-01,  2.7836e-01, -5.1549e-01,  1.6444e-01,\n",
       "         5.2290e-01,  4.6167e-01,  2.9747e-01, -1.6698e-01, -1.2354e-01,\n",
       "         6.3824e-02, -6.2125e-03, -7.0745e-01,  7.4845e-01,  5.0762e-01,\n",
       "        -1.9520e-01,  5.8690e-01,  1.6729e-03,  3.3980e-01, -5.7216e-02,\n",
       "        -2.5909e-01,  1.2762e-01, -2.9704e-01, -2.3820e-01,  4.7426e-02,\n",
       "        -1.6681e-01,  2.9552e-01, -3.7549e-02, -1.8471e-01, -6.9428e-01,\n",
       "         1.8606e-01, -5.0172e-01, -3.5272e-01, -1.9849e-01,  3.4066e-01,\n",
       "         5.3439e-01,  1.0577e+00, -6.5983e-02, -5.4530e-01, -7.1296e-02,\n",
       "         3.2308e-01, -6.2035e-02,  9.2208e-02, -5.0627e-01, -2.1157e-01,\n",
       "         2.9352e-01, -1.6091e-01,  7.6288e-01, -1.6258e-01,  4.4578e-01,\n",
       "        -2.1768e-01,  3.9012e-02,  6.1807e-01, -2.4532e-01,  2.2070e-01,\n",
       "         2.5942e-01,  1.2383e-01, -1.3332e-01,  2.0732e-01,  7.0765e-01,\n",
       "         4.0585e-01, -1.1860e+00,  9.8207e-02,  1.5029e-02, -4.5004e-01,\n",
       "        -2.5370e-01,  4.6468e-01, -2.1910e-01,  3.8951e-01,  4.6833e-02,\n",
       "        -1.3860e-01,  1.0834e-02,  3.1318e-02,  5.0631e-03, -6.9362e-02,\n",
       "        -2.7558e-02,  1.6204e+00, -6.0445e-01, -8.8764e-01,  2.6870e-01,\n",
       "         4.1413e-01, -7.5648e-01, -1.8065e-01,  7.5022e-01,  3.1559e-01,\n",
       "         2.4383e-01, -3.4833e-01, -3.4242e-01, -1.0639e-01,  4.4037e-01,\n",
       "         1.1001e-01, -1.1611e-01, -6.7350e-02,  1.0733e-01, -2.6137e-01,\n",
       "         1.2488e-01,  1.8964e-02, -2.7065e-01, -2.2290e-01,  9.9624e-02,\n",
       "        -3.7188e-02,  6.0160e-02,  3.8521e-01, -6.4455e-01, -2.3414e-01,\n",
       "         2.7962e-01, -3.4191e-02,  2.1691e-01,  2.0646e-01,  3.2041e-01,\n",
       "         1.4724e-01,  2.3428e-01,  6.5114e-02, -1.4536e-01,  2.3181e-02,\n",
       "         2.7250e-01,  2.2470e-01,  4.9756e-01,  2.8387e-01,  1.1017e-01,\n",
       "        -7.5961e-02, -5.1586e-01, -9.1621e-02,  4.4351e-02, -8.1728e-03,\n",
       "         1.7359e-01,  1.2702e-01,  1.6924e-01, -8.3103e-01,  1.5781e-02,\n",
       "        -5.2884e-04,  1.0250e-01,  8.2391e-02, -9.0706e-01, -3.0807e-02,\n",
       "        -3.8721e-01, -5.9538e-01,  9.7281e-02, -4.3904e-01,  1.6242e-02,\n",
       "        -2.2995e-01,  1.0501e-01, -7.5106e-04, -4.8681e-01,  5.8837e-02,\n",
       "         5.4039e-02,  2.2828e-01,  2.2163e-01, -3.0361e-01,  3.7770e-01,\n",
       "         1.1736e-02,  5.1659e-02, -4.3668e-01,  2.1914e-01,  2.0087e-01,\n",
       "         3.9802e-01, -8.9770e-01, -8.2482e-02,  2.1858e-02, -2.3078e-01,\n",
       "        -3.5421e-01,  4.0749e-02, -2.0555e-01,  5.4333e-03, -4.7780e-01,\n",
       "        -2.8086e-01,  2.0115e-01,  7.7899e-02,  1.4119e-01, -4.7543e-02,\n",
       "        -4.0642e-01, -9.8781e-02, -2.4629e-01,  3.3040e-02,  4.2296e-01,\n",
       "        -5.4533e-01,  2.7722e-01, -5.0779e-01,  3.5142e-02, -2.0878e-01,\n",
       "        -3.1299e-01,  4.6083e-01, -2.2129e-01,  4.9161e-01,  1.9228e-01,\n",
       "        -1.1701e-01,  1.9830e-01,  4.8054e-01, -1.5919e-01,  1.5420e-01,\n",
       "        -3.2894e-01, -8.1925e-03,  2.1781e-02, -3.5799e-01,  1.4943e-01,\n",
       "        -7.5653e-02,  3.9595e-01,  1.3097e-01, -6.3085e-01, -1.6285e-01,\n",
       "        -6.6864e-01,  2.6504e-01, -6.2873e-01,  3.8697e-04, -1.1489e-01,\n",
       "         1.1465e-01,  2.4036e-01,  9.2392e-02,  1.0463e-01, -1.7822e-01,\n",
       "        -1.6285e-02, -3.3828e-01, -3.3945e-01, -2.7975e-01,  1.7709e-01,\n",
       "         5.5424e-02, -6.1458e-01, -2.2233e-01,  7.5821e-01,  3.8354e-02,\n",
       "         8.4514e-02,  5.4560e-01,  7.2783e-02, -3.4938e-01,  1.4623e-01,\n",
       "         1.7989e-01, -2.2822e-01,  9.3419e-01,  5.7430e-03,  4.4165e-01,\n",
       "         3.1368e-01, -6.0340e-02, -2.5650e-01, -9.7842e-02,  1.4604e-01,\n",
       "        -4.9760e-02, -2.6213e-01, -4.3863e-03,  7.1640e-02, -3.9418e-01,\n",
       "        -3.5506e-02, -2.1411e-01,  1.8924e-02,  3.7431e-01,  1.4683e-01,\n",
       "        -4.4907e-01, -6.7487e-02, -4.6749e-01, -5.3921e-01,  3.9666e-01,\n",
       "        -1.6362e-01, -3.6235e-01,  2.9192e-01, -1.2840e-01,  1.4505e-02,\n",
       "         2.2861e-01, -3.9412e-01,  4.2768e-01,  1.1586e-01, -7.8025e-02,\n",
       "         3.0860e-01,  1.2840e-01, -4.3787e-01,  7.1750e-01,  3.2716e-01,\n",
       "         8.3862e-02,  4.3643e-01,  4.3428e-01, -4.6911e-01, -5.7997e-02,\n",
       "        -4.7144e-01, -4.2545e-01, -1.3124e-01, -2.3851e-01, -3.3528e-01,\n",
       "         2.9251e-01,  4.1427e-02, -5.5408e-01, -9.0104e-02,  8.3241e-02,\n",
       "        -2.6976e-01, -1.2086e-01,  2.0482e-01, -4.6330e-01, -3.8788e-01,\n",
       "         1.2469e-01, -4.4502e-01,  1.1275e-01,  5.8602e-02,  1.2323e-02,\n",
       "        -1.9823e-01, -4.3178e-02,  1.5686e-01, -3.6768e-01,  8.9107e-02,\n",
       "         8.4543e-02, -3.7897e-02,  1.5394e-01, -2.1197e-01, -1.6185e-01,\n",
       "        -7.7083e-02, -2.1049e-01, -1.5097e-01,  2.0359e-01,  2.4072e-01,\n",
       "        -2.5632e-01, -6.1709e-01, -1.9404e-01, -5.9175e-02, -6.4661e-01,\n",
       "        -2.3497e-01, -2.2523e-01,  4.6893e-03,  5.1832e-01, -2.1251e-01,\n",
       "         4.6777e-03,  2.7804e-01,  4.4101e-01, -1.2368e-02,  1.1713e-01,\n",
       "        -5.1940e-01, -8.8681e-01,  6.1834e-01,  1.4046e-01,  2.5495e-01,\n",
       "         8.2354e-01, -2.3325e-01,  1.7444e-01,  5.4384e-02,  3.2411e-01,\n",
       "        -7.1570e-02,  1.2053e-01,  3.5698e-02,  2.6183e-01, -3.5587e-01,\n",
       "        -2.6045e-01, -2.6078e-01, -3.1329e-01, -3.1231e-01, -1.2140e-02,\n",
       "        -1.2697e-01, -4.1535e-02, -3.8970e-01,  8.4559e-02,  2.3368e-03,\n",
       "        -4.7470e-02, -4.7473e-02,  4.4915e-01, -4.0883e-01, -2.3382e-01,\n",
       "        -2.1030e-03, -1.3033e-01,  5.5369e-01,  5.0290e-02, -2.1603e-01,\n",
       "         4.0952e-01, -1.2175e-01,  2.3254e-01, -6.2730e-02,  3.3170e-01,\n",
       "         1.5514e-01, -8.5978e-02, -1.2891e-01, -4.4992e-01,  6.3192e-01,\n",
       "         5.2864e-01,  5.5625e-01,  6.0436e-01, -1.4084e-01, -3.3886e-02,\n",
       "         3.3850e-01, -3.6229e-01, -2.4427e-01,  2.5061e-03, -3.0842e-01,\n",
       "         4.7709e-02,  4.2391e-01,  9.5898e-02, -2.5999e-02,  5.2000e-01,\n",
       "        -1.1714e-01,  8.0191e-02,  5.1162e-01,  5.2736e-02, -3.7691e-01,\n",
       "         2.6196e-01, -3.7113e-01, -9.2829e-03,  2.2195e-01, -1.3414e-01,\n",
       "        -2.0730e-01,  8.6191e-02,  1.6639e-01,  8.9128e-02, -1.5148e-01,\n",
       "         3.6782e-01,  3.3247e-01,  4.6237e-01, -2.3435e-01,  5.7504e-02,\n",
       "        -5.4347e-02, -1.4533e-01, -1.3381e-01,  6.2137e-01, -5.9748e-01,\n",
       "        -1.8775e-01, -9.3899e-01,  1.2892e-01,  2.2941e-01, -2.1180e-01,\n",
       "        -3.4718e-01, -1.4208e-01, -1.1624e-01, -1.3220e-01,  2.6303e-02,\n",
       "         6.1109e-01, -4.0764e-01,  5.9745e-03,  6.2629e-01, -6.5011e-01,\n",
       "        -4.8020e-01, -3.6668e-02, -3.5849e-01, -5.0177e-01,  1.6122e-01,\n",
       "        -1.3823e-01,  4.4566e-01, -1.5366e-01, -3.1749e-01,  5.7365e-01,\n",
       "         1.1394e-01,  8.3249e-02,  1.6167e-01, -2.9282e-02, -2.5932e-01,\n",
       "         7.2224e-02, -1.9747e+00,  1.1958e-01, -1.9011e-01,  3.9682e-01,\n",
       "        -5.0714e-01, -5.7638e-01,  2.6756e-01, -9.5236e-02,  1.0597e-01,\n",
       "         3.8325e-01, -5.6474e-01, -1.8380e-01, -7.3767e-02,  8.3895e-01,\n",
       "         1.8057e-01,  3.2826e-01,  3.6069e-01,  8.4377e-01,  3.3022e-02,\n",
       "        -9.7213e-02,  2.6318e-01, -1.6147e-01, -8.6078e-01, -5.7384e-01,\n",
       "         1.8777e-01, -5.3465e-01,  3.1616e-02, -1.9919e-01, -8.0515e-03,\n",
       "        -5.7713e-01, -3.4824e-01, -2.5570e-03, -8.0889e-02, -3.7977e-02,\n",
       "        -6.9105e-02,  4.8152e-01, -1.6041e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "sentence_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
