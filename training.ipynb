{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import TransformersModel\n",
    "\n",
    "from preprocess import ProcessGoEmotions, TokenizeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
      "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
      "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
      "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
      "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
      "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
      "       'remorse', 'sadness', 'surprise', 'neutral', 'emotions',\n",
      "       'emotion_category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "PATH1 = \"./data/full_dataset/goemotions_1.csv\"\n",
    "PATH2 = \"./data/full_dataset/goemotions_2.csv\"\n",
    "PATH3 = \"./data/full_dataset/goemotions_3.csv\"\n",
    "\n",
    "# LABEL = \"emotions\" # all 28 labels\n",
    "LABEL = \"emotion_category\" # positive negative ambiguous and neutral\n",
    "\n",
    "process = ProcessGoEmotions(label_choice=LABEL)\n",
    "train_dataset, test_dataset = process.get_datasets(paths=[PATH1], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id',\n",
       "       'created_utc', 'rater_id', 'example_very_unclear', 'admiration',\n",
       "       'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion',\n",
       "       'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust',\n",
       "       'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy',\n",
       "       'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
       "       'remorse', 'sadness', 'surprise', 'neutral'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(PATH1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    }
   ],
   "source": [
    "tonekizer = TokenizeDataset(train_dataset)\n",
    "tokenized_train = tonekizer.tokenize_process(tokenizer_name=\"bert-base-cased\")\n",
    "\n",
    "tonekizer = TokenizeDataset(test_dataset)\n",
    "tokenized_test = tonekizer.tokenize_process(tokenizer_name=\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_train.shuffle(seed=42).select(range(200))\n",
    "small_eval_dataset = tokenized_test.shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "transformers_model = TransformersModel(\n",
    "    optimizer_name=\"AdamW\",\n",
    "    num_epochs=num_epochs,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    num_labels=len(tokenized_train['labels'].unique()),\n",
    "    model_name=\"bert-base-cased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [08:10<00:00,  6.54s/it, loss=tensor(0.5492, grad_fn=<NllLossBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "transformers_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    report = classification_report(labels, predictions, digits=4, output_dict=True)\n",
    "    return {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"class_0_f1\": report[\"0\"][\"f1-score\"],\n",
    "        \"class_1_f1\": report[\"1\"][\"f1-score\"],\n",
    "        \"class_2_f1\": report[\"2\"][\"f1-score\"],\n",
    "        \"class_3_f1\": report[\"3\"][\"f1-score\"],\n",
    "        \"class_4_f1\": report[\"4\"][\"f1-score\"],\n",
    "        \"class_5_f1\": report[\"5\"][\"f1-score\"],\n",
    "        \"class_6_f1\": report[\"6\"][\"f1-score\"],\n",
    "        \"class_7_f1\": report[\"7\"][\"f1-score\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "\n",
    "device = transformers_model.device\n",
    "model = transformers_model.model\n",
    "\n",
    "def model_evaluate(model, device, metric = \"accuracy\"):\n",
    "    metric = evaluate.load(metric)\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    return metric.compute()\n",
    "\n",
    "def model_evaluate_custom(model, device):\n",
    "    all_predictions, all_references = [], []\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions += list(predictions.numpy())\n",
    "        all_references += list(batch[\"labels\"].numpy())\n",
    "    return all_predictions, all_references\n",
    "\n",
    "predictions, references = model_evaluate_custom(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(i) for i in tokenized_train['labels'].unique().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cha/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/cha/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/cha/opt/anaconda3/envs/anlp/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "mapping = {0: \"neutral\", 1: \"ambiguous\", 2: \"negative\", 3: \"positive\"}\n",
    "report = classification_report(references, predictions, target_names=[f\"{mapping[i]}\" for i in tokenized_train['labels'].unique().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.00      0.00      0.00        19\n",
      "   ambiguous       0.27      0.32      0.29        57\n",
      "    negative       0.38      0.28      0.32        50\n",
      "    positive       0.52      0.68      0.58        74\n",
      "\n",
      "    accuracy                           0.41       200\n",
      "   macro avg       0.29      0.32      0.30       200\n",
      "weighted avg       0.36      0.41      0.38       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.41}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(metric=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.35}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
